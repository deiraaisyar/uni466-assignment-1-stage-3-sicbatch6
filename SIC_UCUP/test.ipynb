{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import paho.mqtt.publish as publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840615c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 63.6ms\n",
      "Speed: 2.4ms preprocess, 63.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 49.0ms\n",
      "Speed: 1.4ms preprocess, 49.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 43.1ms\n",
      "Speed: 1.3ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 40.8ms\n",
      "Speed: 1.7ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.2ms\n",
      "Speed: 1.9ms preprocess, 36.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 45.6ms\n",
      "Speed: 1.8ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.3ms\n",
      "Speed: 1.4ms preprocess, 39.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 1.6ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.0ms\n",
      "Speed: 1.4ms preprocess, 35.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 41.1ms\n",
      "Speed: 1.7ms preprocess, 41.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 1.8ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.5ms\n",
      "Speed: 1.8ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.0ms\n",
      "Speed: 1.4ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 38.4ms\n",
      "Speed: 0.9ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.5ms\n",
      "Speed: 1.2ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 1.8ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.1ms\n",
      "Speed: 1.3ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.9ms\n",
      "Speed: 1.9ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 cat, 40.1ms\n",
      "Speed: 1.3ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.4ms\n",
      "Speed: 1.0ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 38.3ms\n",
      "Speed: 1.6ms preprocess, 38.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 1.9ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 car, 38.7ms\n",
      "Speed: 1.6ms preprocess, 38.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.4ms\n",
      "Speed: 1.8ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.4ms\n",
      "Speed: 1.2ms preprocess, 36.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 1.5ms preprocess, 37.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.9ms\n",
      "Speed: 2.1ms preprocess, 37.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 0.9ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 36.4ms\n",
      "Speed: 1.7ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 1.4ms preprocess, 38.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 1.3ms preprocess, 36.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 37.4ms\n",
      "Speed: 1.7ms preprocess, 37.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 2.3ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 38.9ms\n",
      "Speed: 1.7ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 37.1ms\n",
      "Speed: 1.7ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.7ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 1.4ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 36.8ms\n",
      "Speed: 1.5ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.0ms\n",
      "Speed: 1.6ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 39.9ms\n",
      "Speed: 1.6ms preprocess, 39.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.4ms\n",
      "Speed: 1.8ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 37.4ms\n",
      "Speed: 2.1ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 1.9ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.4ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.9ms\n",
      "Speed: 1.3ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 truck, 1 umbrella, 37.9ms\n",
      "Speed: 1.8ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 truck, 1 umbrella, 38.8ms\n",
      "Speed: 1.8ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.4ms\n",
      "Speed: 2.1ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 traffic light, 38.3ms\n",
      "Speed: 2.1ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 1.9ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 39.0ms\n",
      "Speed: 1.9ms preprocess, 39.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.4ms\n",
      "Speed: 1.8ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 sports ball, 39.2ms\n",
      "Speed: 1.2ms preprocess, 39.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.5ms\n",
      "Speed: 1.7ms preprocess, 42.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 2.1ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 36.9ms\n",
      "Speed: 1.5ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.7ms\n",
      "Speed: 1.5ms preprocess, 38.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.2ms\n",
      "Speed: 1.6ms preprocess, 38.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 1.7ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.6ms\n",
      "Speed: 1.4ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 1.9ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.6ms\n",
      "Speed: 1.4ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.2ms\n",
      "Speed: 1.6ms preprocess, 40.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 1.7ms preprocess, 36.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 1.8ms preprocess, 37.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.0ms\n",
      "Speed: 1.7ms preprocess, 36.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 2.3ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.1ms\n",
      "Speed: 1.7ms preprocess, 36.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.5ms\n",
      "Speed: 1.8ms preprocess, 40.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 3 persons, 40.0ms\n",
      "Speed: 1.4ms preprocess, 40.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 52.5ms\n",
      "Speed: 2.0ms preprocess, 52.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 1.7ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 1.2ms preprocess, 36.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 umbrella, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 2.0ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 47.3ms\n",
      "Speed: 1.4ms preprocess, 47.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 car, 38.6ms\n",
      "Speed: 1.8ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 41.6ms\n",
      "Speed: 1.3ms preprocess, 41.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 1.3ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.9ms preprocess, 37.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 45.9ms\n",
      "Speed: 1.4ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.1ms\n",
      "Speed: 1.8ms preprocess, 40.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 1.7ms preprocess, 38.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 38.8ms\n",
      "Speed: 1.2ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 vase, 40.1ms\n",
      "Speed: 1.2ms preprocess, 40.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 1.4ms preprocess, 37.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 36.0ms\n",
      "Speed: 1.4ms preprocess, 36.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 39.6ms\n",
      "Speed: 1.6ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.2ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.4ms\n",
      "Speed: 1.6ms preprocess, 39.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.5ms preprocess, 36.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 1.7ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 1.2ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 42.8ms\n",
      "Speed: 1.1ms preprocess, 42.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.0ms\n",
      "Speed: 1.8ms preprocess, 40.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 2.0ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 vase, 38.6ms\n",
      "Speed: 1.5ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 33.9ms\n",
      "Speed: 1.6ms preprocess, 33.9ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.2ms\n",
      "Speed: 1.4ms preprocess, 36.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 vase, 39.3ms\n",
      "Speed: 2.0ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 39.5ms\n",
      "Speed: 1.8ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.2ms\n",
      "Speed: 1.6ms preprocess, 36.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 1.6ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.7ms\n",
      "Speed: 1.3ms preprocess, 36.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.7ms\n",
      "Speed: 1.6ms preprocess, 36.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 34.5ms\n",
      "Speed: 1.4ms preprocess, 34.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 42.2ms\n",
      "Speed: 2.3ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 35.2ms\n",
      "Speed: 1.7ms preprocess, 35.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.5ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 2.4ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 1.7ms preprocess, 36.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.0ms\n",
      "Speed: 1.3ms preprocess, 35.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.8ms\n",
      "Speed: 1.0ms preprocess, 35.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 2.1ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 36.5ms\n",
      "Speed: 2.1ms preprocess, 36.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 1.7ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.5ms\n",
      "Speed: 1.6ms preprocess, 36.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.5ms\n",
      "Speed: 1.5ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 1.6ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.5ms\n",
      "Speed: 1.7ms preprocess, 41.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.8ms\n",
      "Speed: 1.2ms preprocess, 35.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 1.6ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 40.4ms\n",
      "Speed: 1.5ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.2ms\n",
      "Speed: 1.5ms preprocess, 37.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 36.7ms\n",
      "Speed: 1.7ms preprocess, 36.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.4ms\n",
      "Speed: 1.8ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.9ms preprocess, 36.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.8ms\n",
      "Speed: 1.6ms preprocess, 36.8ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 40.4ms\n",
      "Speed: 1.6ms preprocess, 40.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 1.6ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 37.2ms\n",
      "Speed: 1.4ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.6ms\n",
      "Speed: 1.4ms preprocess, 36.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.9ms\n",
      "Speed: 1.7ms preprocess, 35.9ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.0ms\n",
      "Speed: 1.3ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 38.5ms\n",
      "Speed: 1.5ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.5ms\n",
      "Speed: 2.1ms preprocess, 35.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.9ms\n",
      "Speed: 1.5ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.5ms\n",
      "Speed: 1.9ms preprocess, 36.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 38.6ms\n",
      "Speed: 1.7ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 40.4ms\n",
      "Speed: 1.6ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.3ms\n",
      "Speed: 1.6ms preprocess, 36.3ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.5ms\n",
      "Speed: 1.6ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.8ms\n",
      "Speed: 2.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.9ms\n",
      "Speed: 1.2ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.4ms\n",
      "Speed: 1.2ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.6ms\n",
      "Speed: 2.0ms preprocess, 35.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 39.0ms\n",
      "Speed: 1.6ms preprocess, 39.0ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.8ms\n",
      "Speed: 1.4ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 2.1ms preprocess, 37.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 38.8ms\n",
      "Speed: 1.5ms preprocess, 38.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 1.2ms preprocess, 39.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.6ms\n",
      "Speed: 1.8ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 1.9ms preprocess, 36.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.9ms\n",
      "Speed: 1.3ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.8ms\n",
      "Speed: 1.8ms preprocess, 34.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.2ms\n",
      "Speed: 1.6ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 36.4ms\n",
      "Speed: 1.4ms preprocess, 36.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.3ms\n",
      "Speed: 1.7ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 1.5ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.4ms\n",
      "Speed: 1.6ms preprocess, 38.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 1.7ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.8ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.7ms preprocess, 37.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.3ms\n",
      "Speed: 1.6ms preprocess, 38.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.9ms\n",
      "Speed: 1.8ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.8ms\n",
      "Speed: 1.5ms preprocess, 36.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 vase, 37.9ms\n",
      "Speed: 1.7ms preprocess, 37.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.9ms\n",
      "Speed: 1.5ms preprocess, 37.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.7ms\n",
      "Speed: 1.9ms preprocess, 34.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 1.5ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 1.4ms preprocess, 37.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.4ms\n",
      "Speed: 1.2ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.8ms\n",
      "Speed: 1.4ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.6ms\n",
      "Speed: 1.2ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 0.9ms preprocess, 38.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 0.9ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 sports ball, 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 40.1ms\n",
      "Speed: 2.1ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 1.6ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 36.4ms\n",
      "Speed: 1.6ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 1.2ms preprocess, 36.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 35.4ms\n",
      "Speed: 1.3ms preprocess, 35.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 car, 40.2ms\n",
      "Speed: 1.6ms preprocess, 40.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.4ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.0ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.3ms\n",
      "Speed: 1.7ms preprocess, 38.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 1.2ms preprocess, 36.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.7ms\n",
      "Speed: 1.2ms preprocess, 36.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.6ms\n",
      "Speed: 1.8ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 1.3ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.8ms\n",
      "Speed: 2.2ms preprocess, 36.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 36.3ms\n",
      "Speed: 1.6ms preprocess, 36.3ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.0ms\n",
      "Speed: 2.3ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 39.9ms\n",
      "Speed: 1.4ms preprocess, 39.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.4ms\n",
      "Speed: 1.6ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.2ms\n",
      "Speed: 2.0ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.1ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 1.6ms preprocess, 36.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.0ms\n",
      "Speed: 2.2ms preprocess, 38.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.8ms\n",
      "Speed: 1.4ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.6ms\n",
      "Speed: 2.0ms preprocess, 36.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 35.3ms\n",
      "Speed: 1.0ms preprocess, 35.3ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.1ms\n",
      "Speed: 1.6ms preprocess, 35.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 1.4ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.7ms\n",
      "Speed: 0.9ms preprocess, 35.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.7ms\n",
      "Speed: 1.5ms preprocess, 34.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.7ms\n",
      "Speed: 1.6ms preprocess, 35.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.3ms\n",
      "Speed: 2.2ms preprocess, 39.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 41.1ms\n",
      "Speed: 2.3ms preprocess, 41.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 35.1ms\n",
      "Speed: 1.8ms preprocess, 35.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.7ms\n",
      "Speed: 0.9ms preprocess, 34.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 35.6ms\n",
      "Speed: 1.6ms preprocess, 35.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.4ms\n",
      "Speed: 1.7ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.9ms\n",
      "Speed: 1.6ms preprocess, 34.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.1ms\n",
      "Speed: 1.7ms preprocess, 37.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.5ms\n",
      "Speed: 1.7ms preprocess, 38.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.9ms\n",
      "Speed: 1.3ms preprocess, 34.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.2ms\n",
      "Speed: 1.7ms preprocess, 38.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.8ms\n",
      "Speed: 2.0ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 37.8ms\n",
      "Speed: 1.6ms preprocess, 37.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.1ms\n",
      "Speed: 1.3ms preprocess, 34.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 39.9ms\n",
      "Speed: 1.6ms preprocess, 39.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.9ms\n",
      "Speed: 1.4ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 1.7ms preprocess, 39.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.3ms\n",
      "Speed: 1.6ms preprocess, 36.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 36.5ms\n",
      "Speed: 1.7ms preprocess, 36.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 2.1ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.7ms\n",
      "Speed: 1.8ms preprocess, 37.7ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 38.0ms\n",
      "Speed: 1.7ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 34.0ms\n",
      "Speed: 1.4ms preprocess, 34.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 2.2ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 3 persons, 36.1ms\n",
      "Speed: 1.2ms preprocess, 36.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 3 persons, 37.4ms\n",
      "Speed: 1.9ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 38.7ms\n",
      "Speed: 1.6ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 49.4ms\n",
      "Speed: 1.8ms preprocess, 49.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 1 person, 46.4ms\n",
      "Speed: 1.3ms preprocess, 46.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 2 persons, 39.2ms\n",
      "Speed: 1.4ms preprocess, 39.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[ALERT] Object too close!\n",
      "\n",
      "0: 480x640 (no detections), 37.4ms\n",
      "Speed: 1.4ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.1ms\n",
      "Speed: 1.6ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.1ms\n",
      "Speed: 1.5ms preprocess, 36.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 38.9ms\n",
      "Speed: 1.2ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.6ms\n",
      "Speed: 1.7ms preprocess, 40.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 37.5ms\n",
      "Speed: 1.5ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 40.6ms\n",
      "Speed: 1.4ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.3ms\n",
      "Speed: 1.8ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 46.7ms\n",
      "Speed: 2.2ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to grab frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m boxes:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\predictor.py:216\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\predictor.py:362\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen:\n\u001b[0;32m    361\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(x\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m profilers)  \u001b[38;5;66;03m# speeds per image\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mLOGGER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpeed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_crop:\n\u001b[0;32m    367\u001b[0m     nl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels/*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)))  \u001b[38;5;66;03m# number of labels\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1489\u001b[0m, in \u001b[0;36mLogger.info\u001b[1;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'INFO'.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;124;03mlogger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=True)\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(INFO):\n\u001b[1;32m-> 1489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1634\u001b[0m, in \u001b[0;36mLogger._log\u001b[1;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m   1632\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeRecord(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, level, fn, lno, msg, args,\n\u001b[0;32m   1633\u001b[0m                          exc_info, func, extra, sinfo)\n\u001b[1;32m-> 1634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1644\u001b[0m, in \u001b[0;36mLogger.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;124;03mCall the handlers for the specified record.\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m \n\u001b[0;32m   1640\u001b[0m \u001b[38;5;124;03mThis method is used for unpickled records received from a socket, as\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;124;03mwell as those created locally. Logger-level filtering is applied.\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisabled) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter(record):\n\u001b[1;32m-> 1644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallHandlers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1706\u001b[0m, in \u001b[0;36mLogger.callHandlers\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1704\u001b[0m     found \u001b[38;5;241m=\u001b[39m found \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m hdlr\u001b[38;5;241m.\u001b[39mlevel:\n\u001b[1;32m-> 1706\u001b[0m         \u001b[43mhdlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[0;32m   1708\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m#break out\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:978\u001b[0m, in \u001b[0;36mHandler.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1114\u001b[0m, in \u001b[0;36mStreamHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m     stream\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminator)\n\u001b[1;32m-> 1114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m:  \u001b[38;5;66;03m# See issue 36272\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py:1094\u001b[0m, in \u001b[0;36mStreamHandler.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1094\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # yolov8n = nano, fast\n",
    "\n",
    "# ESP32-CAM stream URL\n",
    "stream_url = \"http://192.168.1.229:81/stream\"  # Replace with your IP\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "def trigger_alert():\n",
    "    UBIDOTS_BROKER = \"industrial.api.ubidots.com\"\n",
    "    UBIDOTS_TOKEN = \"BBUS-iYTamlBTLRQ8di2mUMohiW4ErEmBGf\"\n",
    "    DEVICE_LABEL = \"hazard-node\"\n",
    "    VARIABLE_LABEL = \"alert\"\n",
    "\n",
    "    topic = f\"/v1.6/devices/{DEVICE_LABEL}/{VARIABLE_LABEL}\"\n",
    "    payload = \"{\\\"value\\\":1}\"  # you can send 0 to turn off\n",
    "\n",
    "    publish.single(\n",
    "        topic,\n",
    "        payload=payload,\n",
    "        hostname=UBIDOTS_BROKER,\n",
    "        port=1883,\n",
    "        auth={'username': UBIDOTS_TOKEN, 'password': ''},\n",
    "    )\n",
    "    print(\"[ALERT SENT] to Ubidots\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        continue\n",
    "\n",
    "    results = model(frame)\n",
    "    boxes = results[0].boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        label = model.names[cls]\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Check for large 'person' = close to camera\n",
    "        if label == \"person\" and area > 50000:\n",
    "            trigger_alert()\n",
    "\n",
    "    # Show annotated frame\n",
    "    annotated = results[0].plot()\n",
    "    cv2.imshow(\"ESP32 YOLO Detection\", annotated)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e223773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (1.9.0)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.1.4)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (9.1.2)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair<6,>=4.0->streamlit) (1.29.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.8 MB 991.0 kB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/9.8 MB 2.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/9.8 MB 3.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/9.8 MB 3.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.8 MB 2.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/9.8 MB 2.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/9.8 MB 2.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/9.8 MB 2.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.8/9.8 MB 2.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/9.8 MB 2.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/9.8 MB 2.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.1/9.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.1/9.8 MB 1.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.2/9.8 MB 1.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.3/9.8 MB 1.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.5/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.8/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.8/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.0/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/9.8 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.2/9.8 MB 1.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.3/9.8 MB 1.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.4/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.5/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.6/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.7/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.8/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.9/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.0/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.0/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.1/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.2/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.2/9.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.3/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.3/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.4/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.5/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.5/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.6/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.7/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.7/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.8/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.9/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.0/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.0/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.1/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.1/9.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.3/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.4/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.5/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.5/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.6/9.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 4.7/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.8/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.9/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.9/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.1/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.2/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.3/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.4/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.5/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.6/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.7/9.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.8/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.9/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.0/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.1/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.2/9.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.3/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.4/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.5/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.7/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.8/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.9/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.0/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.1/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.1/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.2/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.2/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.4/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.5/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.5/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.7/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.8/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.8/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.9/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.0/9.8 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.1/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.2/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.3/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.5/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.6/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.1/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.3/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 92.2/731.2 kB 1.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 174.1/731.2 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 266.2/731.2 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 368.6/731.2 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 450.6/731.2 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 553.0/731.2 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 645.1/731.2 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  727.0/731.2 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 731.2/731.2 kB 1.9 MB/s eta 0:00:00\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.1/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/6.9 MB 1.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/6.9 MB 1.0 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/6.9 MB 1.0 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/6.9 MB 1.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.4/6.9 MB 1.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.4/6.9 MB 1.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/6.9 MB 1.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.6/6.9 MB 1.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.6/6.9 MB 1.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.7/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.7/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.8/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.9/6.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.9/6.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.0/6.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.1/6.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.1/6.9 MB 1.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.2/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.3/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.3/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.4/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.5/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.9 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.7/6.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.8/6.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.8/6.9 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.9/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.0/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.2/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.3/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.4/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.4/6.9 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.5/6.9 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.6/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.7/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.8/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.9/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.0/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.1/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.1/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.2/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.3/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.4/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.5/6.9 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.6/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.7/6.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.8/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.1/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.1/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.2/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.3/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.4/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.5/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.6/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.7/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.8/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.9/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.0/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.0/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.1/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.1/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.2/6.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.3/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.3/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.4/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.6/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.6/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.7/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.9/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.1/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.1/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.4/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.5/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.6/6.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.7/6.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.7/6.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, smmap, cachetools, pydeck, jsonschema-specifications, gitdb, jsonschema, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.5.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 pydeck-0.9.1 smmap-5.0.2 streamlit-1.44.1 toml-0.10.2 watchdog-6.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\hp\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import paho.mqtt.publish as publish\n",
    "from datetime import datetime\n",
    "\n",
    "# === CONFIG ===\n",
    "STREAM_URL = \"http://192.168.1.229:81/stream\"  # Replace with your ESP32-CAM IP\n",
    "UBIDOTS_TOKEN = \"BBUS-iYTamlBTLRQ8di2mUMohiW4ErEmBGf\"           # Replace with your token\n",
    "UBIDOTS_BROKER = \"industrial.api.ubidots.com\"\n",
    "DEVICE_LABEL = \"hazard-node\"\n",
    "VARIABLE_LABEL = \"alert\"\n",
    "\n",
    "# === STREAMLIT UI ===\n",
    "st.set_page_config(page_title=\"ESP32-CAM Hazard Detector\", layout=\"wide\")\n",
    "st.title(\" ESP32-CAM Object Proximity Alert\")\n",
    "\n",
    "confidence_thresh = st.sidebar.slider(\"Confidence Threshold\", 0.1, 1.0, 0.5)\n",
    "area_thresh = st.sidebar.slider(\"Bounding Box Area Threshold\", 10000, 150000, 50000)\n",
    "\n",
    "# Manual Trigger Button\n",
    "if st.sidebar.button(\" Manually Trigger Alert\"):\n",
    "    topic = f\"/v1.6/devices/{DEVICE_LABEL}/{VARIABLE_LABEL}\"\n",
    "    payload = \"{\\\"value\\\":1}\"\n",
    "    publish.single(topic, payload, hostname=UBIDOTS_BROKER, port=1883,\n",
    "                   auth={'username': UBIDOTS_TOKEN, 'password': ''})\n",
    "    st.sidebar.success(\"Alert sent manually!\")\n",
    "\n",
    "# Load YOLOv8\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "frame_holder = st.empty()\n",
    "log_holder = st.empty()\n",
    "logs = []\n",
    "\n",
    "def trigger_alert():\n",
    "    topic = f\"/v1.6/devices/{DEVICE_LABEL}/{VARIABLE_LABEL}\"\n",
    "    payload = \"{\\\"value\\\":1}\"\n",
    "    publish.single(topic, payload, hostname=UBIDOTS_BROKER, port=1883,\n",
    "                   auth={'username': UBIDOTS_TOKEN, 'password': ''})\n",
    "    st.toast(\" ALERT: Object Too Close!\")\n",
    "\n",
    "# Open camera stream\n",
    "cap = cv2.VideoCapture(STREAM_URL)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        st.warning(\" Failed to grab frame from camera.\")\n",
    "        break\n",
    "\n",
    "    results = model(frame)[0]\n",
    "    annotated = results.plot()\n",
    "\n",
    "    for box in results.boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        label = model.names[cls]\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        if label == \"person\" and area > area_thresh:\n",
    "            trigger_alert()\n",
    "            logs.append(f\"[{datetime.now().strftime('%H:%M:%S')}]  {label.upper()} - Area: {int(area)}\")\n",
    "\n",
    "    frame_holder.image(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB), channels=\"RGB\")\n",
    "    if logs:\n",
    "        log_holder.markdown(\"###  Detection Log\\n\" + \"\\n\".join(logs[-10:]))\n",
    "\n",
    "    if not st.runtime.exists(): break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de0f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
